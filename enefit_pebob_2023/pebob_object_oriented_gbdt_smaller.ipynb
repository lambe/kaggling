{"cells":[{"cell_type":"markdown","metadata":{},"source":["**Baseline was taken from the beautiful** [Enefit Generic Notebook](https://www.kaggle.com/code/greysky/enefit-generic-notebook)  **and** [Enefit - lgb with regression_l1 objective](https://www.kaggle.com/code/davero/enefit-lgb-with-regression-l1-objective) 🙏\n","\n","**Some baseline techniques and competition mechanics are demonstrated in the** [Explain Dataset and Baseline](https://www.kaggle.com/code/vitalykudelya/explain-dataset-and-baseline)"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-01-07T03:11:29.475639Z","iopub.status.busy":"2024-01-07T03:11:29.475229Z","iopub.status.idle":"2024-01-07T03:11:32.202498Z","shell.execute_reply":"2024-01-07T03:11:32.201445Z","shell.execute_reply.started":"2024-01-07T03:11:29.475601Z"},"trusted":true},"outputs":[],"source":["import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","import os\n","import gc\n","import holidays\n","import pickle\n","\n","import numpy as np\n","import pandas as pd\n","import polars as pl\n","# import plotly.express as px\n","\n","import optuna\n","import lightgbm as lgb\n","from sklearn.ensemble import VotingRegressor\n","from sklearn.model_selection import cross_val_score, cross_validate\n"]},{"cell_type":"markdown","metadata":{},"source":["# Classes"]},{"cell_type":"markdown","metadata":{},"source":["### DataStorage"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-07T03:11:32.205098Z","iopub.status.busy":"2024-01-07T03:11:32.204716Z","iopub.status.idle":"2024-01-07T03:11:32.229457Z","shell.execute_reply":"2024-01-07T03:11:32.228489Z","shell.execute_reply.started":"2024-01-07T03:11:32.205064Z"},"trusted":true},"outputs":[],"source":["class DataStorage:\n","    # root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n","    root = \"competition_data\"\n","\n","    data_cols = [\n","        \"target\",\n","        \"county\",\n","        \"is_business\",\n","        \"product_type\",\n","        \"is_consumption\",\n","        \"datetime\",\n","        \"row_id\",\n","    ]\n","    client_cols = [\n","        \"product_type\",\n","        \"county\",\n","        \"eic_count\",\n","        \"installed_capacity\",\n","        \"is_business\",\n","        \"date\",\n","    ]\n","    gas_prices_cols = [\"forecast_date\", \"lowest_price_per_mwh\", \"highest_price_per_mwh\"]\n","    electricity_prices_cols = [\"forecast_date\", \"euros_per_mwh\"]\n","    forecast_weather_cols = [\n","        \"latitude\",\n","        \"longitude\",\n","        \"hours_ahead\",\n","        \"temperature\",\n","        \"dewpoint\",\n","        \"cloudcover_high\",\n","        \"cloudcover_low\",\n","        \"cloudcover_mid\",\n","        \"cloudcover_total\",\n","        \"10_metre_u_wind_component\",\n","        \"10_metre_v_wind_component\",\n","        \"forecast_datetime\",\n","        \"direct_solar_radiation\",\n","        \"surface_solar_radiation_downwards\",\n","        \"snowfall\",\n","        \"total_precipitation\",\n","    ]\n","    historical_weather_cols = [\n","        \"datetime\",\n","        \"temperature\",\n","        \"dewpoint\",\n","        \"rain\",\n","        \"snowfall\",\n","        \"surface_pressure\",\n","        \"cloudcover_total\",\n","        \"cloudcover_low\",\n","        \"cloudcover_mid\",\n","        \"cloudcover_high\",\n","        \"windspeed_10m\",\n","        \"winddirection_10m\",\n","        \"shortwave_radiation\",\n","        \"direct_solar_radiation\",\n","        \"diffuse_radiation\",\n","        \"latitude\",\n","        \"longitude\",\n","    ]\n","    location_cols = [\"longitude\", \"latitude\", \"county\"]\n","    target_cols = [\n","        \"target\",\n","        \"county\",\n","        \"is_business\",\n","        \"product_type\",\n","        \"is_consumption\",\n","        \"datetime\",\n","    ]\n","\n","    def __init__(self):\n","        self.df_data = pl.read_csv(\n","            os.path.join(self.root, \"train.csv\"),\n","            columns=self.data_cols,\n","            try_parse_dates=True,\n","        )\n","        self.df_client = pl.read_csv(\n","            os.path.join(self.root, \"client.csv\"),\n","            columns=self.client_cols,\n","            try_parse_dates=True,\n","        )\n","        self.df_gas_prices = pl.read_csv(\n","            os.path.join(self.root, \"gas_prices.csv\"),\n","            columns=self.gas_prices_cols,\n","            try_parse_dates=True,\n","        )\n","        self.df_electricity_prices = pl.read_csv(\n","            os.path.join(self.root, \"electricity_prices.csv\"),\n","            columns=self.electricity_prices_cols,\n","            try_parse_dates=True,\n","        )\n","        self.df_forecast_weather = pl.read_csv(\n","            os.path.join(self.root, \"forecast_weather.csv\"),\n","            columns=self.forecast_weather_cols,\n","            try_parse_dates=True,\n","        )\n","        self.df_historical_weather = pl.read_csv(\n","            os.path.join(self.root, \"historical_weather.csv\"),\n","            columns=self.historical_weather_cols,\n","            try_parse_dates=True,\n","        )\n","        self.df_weather_station_to_county_mapping = pl.read_csv(\n","            os.path.join(self.root, \"weather_station_to_county_mapping.csv\"),\n","            columns=self.location_cols,\n","            try_parse_dates=True,\n","        )\n","        self.df_data = self.df_data.filter(\n","            pl.col(\"datetime\") >= pd.to_datetime(\"2022-01-01\")\n","        )\n","        self.df_target = self.df_data.select(self.target_cols)\n","\n","        self.schema_data = self.df_data.schema\n","        self.schema_client = self.df_client.schema\n","        self.schema_gas_prices = self.df_gas_prices.schema\n","        self.schema_electricity_prices = self.df_electricity_prices.schema\n","        self.schema_forecast_weather = self.df_forecast_weather.schema\n","        self.schema_historical_weather = self.df_historical_weather.schema\n","        self.schema_target = self.df_target.schema\n","\n","        self.df_weather_station_to_county_mapping = (\n","            self.df_weather_station_to_county_mapping.with_columns(\n","                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n","                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n","            )\n","        )\n","\n","    def update_with_new_data(\n","        self,\n","        df_new_client,\n","        df_new_gas_prices,\n","        df_new_electricity_prices,\n","        df_new_forecast_weather,\n","        df_new_historical_weather,\n","        df_new_target,\n","    ):\n","        df_new_client = pl.from_pandas(\n","            df_new_client[self.client_cols], schema_overrides=self.schema_client\n","        )\n","        df_new_gas_prices = pl.from_pandas(\n","            df_new_gas_prices[self.gas_prices_cols],\n","            schema_overrides=self.schema_gas_prices,\n","        )\n","        df_new_electricity_prices = pl.from_pandas(\n","            df_new_electricity_prices[self.electricity_prices_cols],\n","            schema_overrides=self.schema_electricity_prices,\n","        )\n","        df_new_forecast_weather = pl.from_pandas(\n","            df_new_forecast_weather[self.forecast_weather_cols],\n","            schema_overrides=self.schema_forecast_weather,\n","        )\n","        df_new_historical_weather = pl.from_pandas(\n","            df_new_historical_weather[self.historical_weather_cols],\n","            schema_overrides=self.schema_historical_weather,\n","        )\n","        df_new_target = pl.from_pandas(\n","            df_new_target[self.target_cols], schema_overrides=self.schema_target\n","        )\n","\n","        self.df_client = pl.concat([self.df_client, df_new_client]).unique(\n","            [\"date\", \"county\", \"is_business\", \"product_type\"]\n","        )\n","        self.df_gas_prices = pl.concat([self.df_gas_prices, df_new_gas_prices]).unique(\n","            [\"forecast_date\"]\n","        )\n","        self.df_electricity_prices = pl.concat(\n","            [self.df_electricity_prices, df_new_electricity_prices]\n","        ).unique([\"forecast_date\"])\n","        self.df_forecast_weather = pl.concat(\n","            [self.df_forecast_weather, df_new_forecast_weather]\n","        ).unique([\"forecast_datetime\", \"latitude\", \"longitude\", \"hours_ahead\"])\n","        self.df_historical_weather = pl.concat(\n","            [self.df_historical_weather, df_new_historical_weather]\n","        ).unique([\"datetime\", \"latitude\", \"longitude\"])\n","        self.df_target = pl.concat([self.df_target, df_new_target]).unique(\n","            [\"datetime\", \"county\", \"is_business\", \"product_type\", \"is_consumption\"]\n","        )\n","\n","    def preprocess_test(self, df_test):\n","        df_test = df_test.rename(columns={\"prediction_datetime\": \"datetime\"})\n","        df_test = pl.from_pandas(\n","            df_test[self.data_cols[1:]], schema_overrides=self.schema_data\n","        )\n","        return df_test\n"]},{"cell_type":"markdown","metadata":{},"source":["### FeaturesGenerator"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-07T03:11:32.231170Z","iopub.status.busy":"2024-01-07T03:11:32.230864Z","iopub.status.idle":"2024-01-07T03:11:32.273305Z","shell.execute_reply":"2024-01-07T03:11:32.272441Z","shell.execute_reply.started":"2024-01-07T03:11:32.231145Z"},"trusted":true},"outputs":[],"source":["class FeaturesGenerator:\n","    def __init__(self, data_storage):\n","        self.data_storage = data_storage\n","\n","    def _add_general_features(self, df_features):\n","        df_features = (\n","            df_features.with_columns(\n","                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n","                pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n","                pl.col(\"datetime\").dt.day().alias(\"day\"),\n","                pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n","                pl.col(\"datetime\").dt.month().alias(\"month\"),\n","                pl.col(\"datetime\").dt.year().alias(\"year\"),\n","            )\n","            .with_columns(\n","                pl.concat_str(\n","                    \"county\",\n","                    \"is_business\",\n","                    \"product_type\",\n","                    \"is_consumption\",\n","                    separator=\"_\",\n","                ).alias(\"segment\"),\n","            )\n","            .with_columns(\n","                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n","                (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n","                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n","                (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n","            )\n","        )\n","        return df_features\n","\n","    def _add_client_features(self, df_features):\n","        df_client = self.data_storage.df_client\n","\n","        df_features = df_features.join(\n","            df_client.with_columns(\n","                (pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n","            ),\n","            on=[\"county\", \"is_business\", \"product_type\", \"date\"],\n","            how=\"left\",\n","        )\n","        return df_features\n","\n","    def _add_forecast_weather_features(self, df_features):\n","        df_forecast_weather = self.data_storage.df_forecast_weather\n","        df_weather_station_to_county_mapping = (\n","            self.data_storage.df_weather_station_to_county_mapping\n","        )\n","\n","        df_forecast_weather = (\n","            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n","            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n","            .drop(\"hours_ahead\")\n","            .with_columns(\n","                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n","                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n","            )\n","            .join(\n","                df_weather_station_to_county_mapping,\n","                how=\"left\",\n","                on=[\"longitude\", \"latitude\"],\n","            )\n","            .drop(\"longitude\", \"latitude\")\n","        )\n","\n","        df_forecast_weather_date = (\n","            df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\")\n","        )\n","\n","        df_forecast_weather_local = (\n","            df_forecast_weather.filter(pl.col(\"county\").is_not_null())\n","            .group_by(\"county\", \"datetime\")\n","            .mean()\n","        )\n","\n","        for hours_lag in [0, 7 * 24]:\n","            df_features = df_features.join(\n","                df_forecast_weather_date.with_columns(\n","                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n","                ),\n","                on=\"datetime\",\n","                how=\"left\",\n","                suffix=f\"_forecast_{hours_lag}h\",\n","            )\n","            df_features = df_features.join(\n","                df_forecast_weather_local.with_columns(\n","                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n","                ),\n","                on=[\"county\", \"datetime\"],\n","                how=\"left\",\n","                suffix=f\"_forecast_local_{hours_lag}h\",\n","            )\n","\n","        return df_features\n","\n","    def _add_historical_weather_features(self, df_features):\n","        df_historical_weather = self.data_storage.df_historical_weather\n","        df_weather_station_to_county_mapping = (\n","            self.data_storage.df_weather_station_to_county_mapping\n","        )\n","\n","        df_historical_weather = (\n","            df_historical_weather.with_columns(\n","                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n","                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n","            )\n","            .join(\n","                df_weather_station_to_county_mapping,\n","                how=\"left\",\n","                on=[\"longitude\", \"latitude\"],\n","            )\n","            .drop(\"longitude\", \"latitude\")\n","        )\n","\n","        df_historical_weather_date = (\n","            df_historical_weather.group_by(\"datetime\").mean().drop(\"county\")\n","        )\n","\n","        df_historical_weather_local = (\n","            df_historical_weather.filter(pl.col(\"county\").is_not_null())\n","            .group_by(\"county\", \"datetime\")\n","            .mean()\n","        )\n","\n","        for hours_lag in [2 * 24, 7 * 24]:\n","            df_features = df_features.join(\n","                df_historical_weather_date.with_columns(\n","                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n","                ),\n","                on=\"datetime\",\n","                how=\"left\",\n","                suffix=f\"_historical_{hours_lag}h\",\n","            )\n","            df_features = df_features.join(\n","                df_historical_weather_local.with_columns(\n","                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n","                ),\n","                on=[\"county\", \"datetime\"],\n","                how=\"left\",\n","                suffix=f\"_historical_local_{hours_lag}h\",\n","            )\n","\n","        for hours_lag in [1 * 24]:\n","            df_features = df_features.join(\n","                df_historical_weather_date.with_columns(\n","                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n","                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n","                )\n","                .filter(pl.col(\"hour\") <= 10)\n","                .drop(\"hour\"),\n","                on=\"datetime\",\n","                how=\"left\",\n","                suffix=f\"_historical_{hours_lag}h\",\n","            )\n","\n","        return df_features\n","\n","    def _add_target_features(self, df_features):\n","        df_target = self.data_storage.df_target\n","\n","        df_target_all_type_sum = (\n","            df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"])\n","            .sum()\n","            .drop(\"product_type\")\n","        )\n","\n","        df_target_all_county_type_sum = (\n","            df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"])\n","            .sum()\n","            .drop(\"product_type\", \"county\")\n","        )\n","\n","        # Add log of target value to df_target\n","        df_target = df_target.with_columns(\n","            pl.col(\"target\").log().alias(\"target_log\")\n","        )\n","\n","        for hours_lag in [\n","            2 * 24,\n","            3 * 24,\n","            4 * 24,\n","            5 * 24,\n","            6 * 24,\n","            7 * 24,\n","            8 * 24,\n","            9 * 24,\n","            10 * 24,\n","            11 * 24,\n","            12 * 24,\n","            13 * 24,\n","            14 * 24,\n","        ]:\n","            df_features = df_features.join(\n","                df_target.with_columns(\n","                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n","                ).rename({\"target\": f\"target_{hours_lag}h\", \"target_log\": f\"target_log_{hours_lag}h\"}),\n","                on=[\n","                    \"county\",\n","                    \"is_business\",\n","                    \"product_type\",\n","                    \"is_consumption\",\n","                    \"datetime\",\n","                ],\n","                how=\"left\",\n","            )\n","\n","        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n","            df_features = df_features.join(\n","                df_target_all_type_sum.with_columns(\n","                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n","                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n","                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],\n","                how=\"left\",\n","            )\n","\n","            df_features = df_features.join(\n","                df_target_all_county_type_sum.with_columns(\n","                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n","                ).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n","                on=[\"is_business\", \"is_consumption\", \"datetime\"],\n","                how=\"left\",\n","                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n","            )\n","\n","        cols_for_stats = [\n","            f\"target_{hours_lag}h\" for hours_lag in [2 * 24, 3 * 24, 4 * 24, 5 * 24]\n","        ] + [\n","            f\"target_log_{hours_lag}h\" for hours_lag in [2 * 24, 3 * 24, 4 * 24, 5 * 24]\n","        ]\n","        df_features = df_features.with_columns(\n","            df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n","            df_features.select(cols_for_stats)\n","            .transpose()\n","            .std()\n","            .transpose()\n","            .to_series()\n","            .alias(f\"target_std\"),\n","        )\n","\n","        for target_prefix, lag_nominator, lag_denomonator in [\n","            (\"target\", 24 * 7, 24 * 14),\n","            (\"target\", 24 * 2, 24 * 9),\n","            (\"target\", 24 * 3, 24 * 10),\n","            (\"target\", 24 * 2, 24 * 3),\n","            (\"target_all_type_sum\", 24 * 2, 24 * 3),\n","            (\"target_all_type_sum\", 24 * 7, 24 * 14),\n","            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),\n","            (\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n","        ]:\n","            df_features = df_features.with_columns(\n","                (\n","                    pl.col(f\"{target_prefix}_{lag_nominator}h\")\n","                    / (pl.col(f\"{target_prefix}_{lag_denomonator}h\") + 1e-3)\n","                ).alias(f\"{target_prefix}_ratio_{lag_nominator}_{lag_denomonator}\")\n","            )\n","\n","        return df_features\n","\n","    def _reduce_memory_usage(self, df_features):\n","        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n","        return df_features\n","\n","    def _drop_columns(self, df_features):\n","        df_features = df_features.drop(\n","            \"date\", \"datetime\", \"hour\", \"dayofyear\"\n","        )\n","        return df_features\n","\n","    def _to_pandas(self, df_features, y):\n","        cat_cols = [\n","            \"county\",\n","            \"is_business\",\n","            \"product_type\",\n","            \"is_consumption\",\n","            \"segment\",\n","            \"weekday\"\n","        ]\n","\n","        if y is not None:\n","            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n","        else:\n","            df_features = df_features.to_pandas()\n","\n","        # Add holidays as a categorical feature\n","        estonian_holidays = holidays.country_holidays('EE', years=range(2021, 2026))\n","        estonian_holidays_keys = list(estonian_holidays.keys())\n","        df_features[\"temp_date\"] = pd.to_datetime(df_features[['year', 'month', 'day']])\n","        df_features['is_holiday'] = df_features[\"temp_date\"].isin(estonian_holidays_keys).astype(int)\n","        df_features.drop(columns=[\"temp_date\"], inplace=True)\n","\n","        df_features = df_features.set_index(\"row_id\")\n","        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n","#         df_features[\"month_cat\"] = df_features[\"month\"].astype(\"category\")\n","\n","        return df_features\n","\n","    def generate_features(self, df_prediction_items):\n","        if \"target\" in df_prediction_items.columns:\n","            df_prediction_items, y = (\n","                df_prediction_items.drop(\"target\"),\n","                df_prediction_items.select(\"target\"),\n","            )\n","        else:\n","            y = None\n","\n","        df_features = df_prediction_items.with_columns(\n","            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n","        )\n","\n","        for add_features in [\n","            self._add_general_features,\n","            self._add_client_features,\n","            self._add_forecast_weather_features,\n","            self._add_historical_weather_features,\n","            self._add_target_features,\n","            self._reduce_memory_usage,\n","            self._drop_columns,\n","        ]:\n","            df_features = add_features(df_features)\n","\n","        df_features = self._to_pandas(df_features, y)\n","\n","        return df_features\n"]},{"cell_type":"markdown","metadata":{},"source":["### Model"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class MonthlyKFold:\n","    def __init__(self, n_splits=3):\n","        self.n_splits = n_splits\n","        \n","    def split(self, X, y, groups=None):\n","        dates = 12 * X[\"year\"] + X[\"month\"]\n","        timesteps = sorted(dates.unique().tolist())\n","        X = X.reset_index()\n","        \n","        for t in timesteps[-self.n_splits:]:\n","            idx_train = X[dates.values < t].index\n","            idx_test = X[dates.values == t].index\n","            \n","            yield idx_train, idx_test\n","            \n","    def get_n_splits(self, X, y, groups=None):\n","        return self.n_splits"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-07T03:11:32.275640Z","iopub.status.busy":"2024-01-07T03:11:32.275357Z","iopub.status.idle":"2024-01-07T03:11:32.290104Z","shell.execute_reply":"2024-01-07T03:11:32.289266Z","shell.execute_reply.started":"2024-01-07T03:11:32.275616Z"},"trusted":true},"outputs":[],"source":["class Model:\n","    def __init__(self, n_models=5):\n","        self.model_consumption_parameters = {\n","            \"n_estimators\": 10,\n","            \"learning_rate\": 0.03,\n","            \"colsample_bytree\": 0.85,\n","            \"colsample_bynode\": 0.65,\n","            \"lambda_l1\": 1.6,\n","            \"lambda_l2\": 10.0,\n","            \"max_depth\": 14,\n","            \"num_leaves\": 750,\n","            \"min_data_in_leaf\": 25,\n","            \"objective\": \"regression_l1\",\n","#             \"device\": \"gpu\"\n","        }\n","\n","        self.model_production_parameters = {\n","            \"n_estimators\": 10,\n","            \"learning_rate\": 0.04,\n","            \"colsample_bytree\": 0.8,\n","            \"colsample_bynode\": 0.55,\n","            \"lambda_l1\": 1.3,\n","            \"lambda_l2\": 9.5,\n","            \"max_depth\": 15,\n","            \"num_leaves\": 1000,\n","            \"min_data_in_leaf\": 20,\n","            \"objective\": \"regression_l1\",\n","#             \"device\": \"gpu\"\n","        }\n","\n","        self.model_consumption = VotingRegressor(\n","            [\n","                (\n","                    f\"consumption_lgb_{i}\",\n","                    lgb.LGBMRegressor(**self.model_consumption_parameters, random_state=i),\n","                )\n","                for i in range(n_models)\n","            ]\n","        )\n","        self.model_production = VotingRegressor(\n","            [\n","                (\n","                    f\"production_lgb_{i}\",\n","                    lgb.LGBMRegressor(**self.model_production_parameters, random_state=i),\n","                )\n","                for i in range(n_models)\n","            ]\n","        )\n","\n","    def fit(self, df_train_features):\n","        mask = df_train_features[\"is_consumption\"] == 1\n","        self.model_consumption.fit(\n","            X=df_train_features[mask].drop(columns=[\"target\"]),\n","            y=df_train_features[mask][\"target\"]\n","        )\n","\n","        mask = df_train_features[\"is_consumption\"] == 0\n","        self.model_production.fit(\n","            X=df_train_features[mask].drop(columns=[\"target\"]),\n","            y=df_train_features[mask][\"target\"]\n","        )\n","\n","    def cross_validate(self, df_train_features):\n","        result = cross_validate(\n","            estimator=lgb.LGBMRegressor(**self.model_consumption_parameters, random_state=42),\n","            X=df_train_features[df_train_features['is_consumption']==1].drop(columns=[\"target\"]), \n","            y=df_train_features[df_train_features['is_consumption']==1][\"target\"],\n","            scoring=\"neg_mean_absolute_error\",\n","            cv=MonthlyKFold(1),\n","        )\n","\n","        print(f\"Fit Time(s): {result['fit_time'].mean():.3f}\")\n","        print(f\"Score Time(s): {result['score_time'].mean():.3f}\")\n","        print(f\"Error(MAE): {-result['test_score'].mean():.3f}\")\n","\n","        result_solar = cross_validate(\n","            estimator=lgb.LGBMRegressor(**self.model_production_parameters, random_state=42),\n","            X=df_train_features[df_train_features['is_consumption']==0].drop(columns=[\"target\"]), \n","            y=df_train_features[df_train_features['is_consumption']==0][\"target\"],\n","            scoring=\"neg_mean_absolute_error\",\n","            cv=MonthlyKFold(1),\n","        )\n","\n","        print(f\"Fit Time(s): {result_solar['fit_time'].mean():.3f}\")\n","        print(f\"Score Time(s): {result_solar['score_time'].mean():.3f}\")\n","        print(f\"Error(MAE): {-result_solar['test_score'].mean():.3f}\")\n","\n","    def predict(self, df_features):\n","        predictions = np.zeros(len(df_features))\n","\n","        mask = df_features[\"is_consumption\"] == 1\n","        predictions[mask.values] = self.model_consumption.predict(\n","            df_features[mask]\n","        ).clip(0)\n","\n","        mask = df_features[\"is_consumption\"] == 0\n","        predictions[mask.values] = self.model_production.predict(\n","            df_features[mask]\n","        ).clip(0)\n","\n","        return predictions\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["class SegmentModel(Model):\n","    def __init__(self, all_segments, n_models=5):\n","        super().__init__(n_models=n_models)\n","\n","        # When setting up the segment models, we need to reduce the number of leaves \n","        # and min data to get a better fit for less data (10k points vs 800k points)\n","        self.model_consumption_seg_parameters = self.model_consumption_parameters.copy()\n","        self.model_consumption_seg_parameters[\"n_estimators\"] = 100\n","        self.model_consumption_seg_parameters[\"num_leaves\"] = 250\n","        self.model_consumption_seg_parameters[\"min_data_in_leaf\"] = 5\n","        self.model_production_seg_parameters = self.model_production_parameters.copy()\n","        self.model_production_seg_parameters[\"n_estimators\"] = 100\n","        self.model_production_seg_parameters[\"num_leaves\"] = 250\n","        self.model_production_seg_parameters[\"min_data_in_leaf\"] = 5\n","\n","        # Set up one model per segment found in the training data\n","        self.all_segments = all_segments\n","        self.consumption_segments = [seg for seg in all_segments if seg.endswith(\"1\")]\n","        self.production_segments = [seg for seg in all_segments if seg.endswith(\"0\")]\n","        self.model_consumption_segments = {\n","            seg: VotingRegressor(\n","                [\n","                (\n","                    f\"consumption_lgb_{i}\",\n","                    lgb.LGBMRegressor(**self.model_consumption_seg_parameters, random_state=i),\n","                )\n","                for i in range(n_models)\n","                ]\n","            )\n","            for seg in self.consumption_segments\n","        }\n","        self.model_production_segments = {\n","            seg: VotingRegressor(\n","                [\n","                (\n","                    f\"production_lgb_{i}\",\n","                    lgb.LGBMRegressor(**self.model_production_seg_parameters, random_state=i),\n","                )\n","                for i in range(n_models)\n","                ]\n","            )\n","            for seg in self.production_segments\n","        }\n","\n","    def fit(self, df_train_features):\n","        # Fit global models\n","        super().fit(df_train_features)\n","        # mask = df_train_features[\"is_consumption\"] == 1\n","        # self.model_consumption.fit(\n","        #     X=df_train_features[mask].drop(columns=[\"target\"]),\n","        #     y=df_train_features[mask][\"target\"]\n","        # )\n","\n","        # mask = df_train_features[\"is_consumption\"] == 0\n","        # self.model_production.fit(\n","        #     X=df_train_features[mask].drop(columns=[\"target\"]),\n","        #     y=df_train_features[mask][\"target\"]\n","        # )\n","\n","        # Fit segment models\n","        for seg in self.all_segments:\n","            seg_df = df_train_features[df_train_features[\"segment\"] == seg]\n","\n","            if seg in self.consumption_segments:\n","                self.model_consumption_segments[seg].fit(\n","                    X=seg_df.drop(columns=[\"target\"]),\n","                    y=seg_df[\"target\"]\n","                )\n","            else:\n","                self.model_production_segments[seg].fit(\n","                    X=seg_df.drop(columns=[\"target\"]),\n","                    y=seg_df[\"target\"]\n","                )\n","\n","    def predict(self, df_features):\n","        predictions = np.zeros(len(df_features))\n","\n","        segs_in_features = df_features[\"segment\"].unique().tolist()\n","        for seg in segs_in_features:\n","            # Use the segment models if the segment is in the training data\n","            # Use the global models otherwise\n","            if seg in self.consumption_segments:\n","                mask = df_features[\"segment\"] == seg\n","                predictions[mask.values] = self.model_consumption_segments[seg].predict(\n","                    df_features[mask]\n","                ).clip(0)\n","            elif seg in self.production_segments:\n","                mask = df_features[\"segment\"] == seg\n","                predictions[mask.values] = self.model_production_segments[seg].predict(\n","                    df_features[mask]\n","                ).clip(0)\n","            else:\n","                # Not sure if I need to account for this possibility, but just in case...\n","                if seg.endswith(\"1\"):\n","                    mask = df_features[\"segment\"] == seg\n","                    predictions[mask.values] = self.model_consumption.predict(\n","                        df_features[mask]\n","                    ).clip(0)\n","                else:\n","                    mask = df_features[\"segment\"] == seg\n","                    predictions[mask.values] = self.model_production.predict(\n","                        df_features[mask]\n","                    ).clip(0)\n","\n","        return predictions\n"]},{"cell_type":"markdown","metadata":{},"source":["# Initialisation"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-07T03:11:32.291685Z","iopub.status.busy":"2024-01-07T03:11:32.291356Z","iopub.status.idle":"2024-01-07T03:11:35.046743Z","shell.execute_reply":"2024-01-07T03:11:35.045941Z","shell.execute_reply.started":"2024-01-07T03:11:32.291660Z"},"trusted":true},"outputs":[],"source":["data_storage = DataStorage()\n","features_generator = FeaturesGenerator(data_storage=data_storage)"]},{"cell_type":"markdown","metadata":{},"source":["# Feature Generation"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-01-07T03:11:35.048662Z","iopub.status.busy":"2024-01-07T03:11:35.047952Z","iopub.status.idle":"2024-01-07T03:11:52.211565Z","shell.execute_reply":"2024-01-07T03:11:52.210707Z","shell.execute_reply.started":"2024-01-07T03:11:35.048623Z"},"trusted":true},"outputs":[],"source":["df_train_features = features_generator.generate_features(data_storage.df_data)\n","df_train_features = df_train_features[df_train_features['target'].notnull()]"]},{"cell_type":"markdown","metadata":{},"source":["## Hyperparameter tuning"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def lgb_p_objective(trial):\n","    params = {\n","        'n_iter'           : 1000,\n","        'verbose'          : -1,\n","        'random_state'     : 42,\n","        'objective'        : 'l1',\n","        'learning_rate'    : trial.suggest_float('learning_rate', 0.01, 0.1),\n","        'colsample_bytree' : trial.suggest_float('colsample_bytree', 0.5, 1.0),\n","        'colsample_bynode' : trial.suggest_float('colsample_bynode', 0.5, 1.0),\n","        'lambda_l1'        : trial.suggest_float('lambda_l1', 1e-2, 10.0),\n","        'lambda_l2'        : trial.suggest_float('lambda_l2', 1e-2, 10.0),\n","        'min_data_in_leaf' : trial.suggest_int('min_data_in_leaf', 4, 128),\n","        'max_depth'        : trial.suggest_int('max_depth', 5, 15),\n","        # 'max_bin'          : trial.suggest_int('max_bin', 32, 1024),\n","        'num_leaves'       : trial.suggest_int('num_leaves', 16, 1024),\n","    }\n","    \n","    model  = lgb.LGBMRegressor(**params)\n","    X, y   = df_train_features[df_train_features['is_consumption']==0].drop(columns=[\"target\"]), df_train_features[df_train_features['is_consumption']==0][\"target\"]\n","    cv     = MonthlyKFold(1)\n","    scores = cross_val_score(model, X, y, cv=cv, scoring='neg_mean_absolute_error')\n","    \n","    return -1 * np.mean(scores)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def lgb_c_objective(trial):\n","    params = {\n","        'n_iter'           : 1000,\n","        'verbose'          : -1,\n","        'random_state'     : 42,\n","        'objective'        : 'l1',\n","        'learning_rate'    : trial.suggest_float('learning_rate', 0.01, 0.1),\n","        'colsample_bytree' : trial.suggest_float('colsample_bytree', 0.5, 1.0),\n","        'colsample_bynode' : trial.suggest_float('colsample_bynode', 0.5, 1.0),\n","        'lambda_l1'        : trial.suggest_float('lambda_l1', 1e-2, 10.0),\n","        'lambda_l2'        : trial.suggest_float('lambda_l2', 1e-2, 10.0),\n","        'min_data_in_leaf' : trial.suggest_int('min_data_in_leaf', 4, 128),\n","        'max_depth'        : trial.suggest_int('max_depth', 5, 15),\n","        # 'max_bin'          : trial.suggest_int('max_bin', 32, 1024),\n","        'num_leaves'       : trial.suggest_int('num_leaves', 16, 1024),\n","    }\n","    \n","    model  = lgb.LGBMRegressor(**params)\n","    X, y   = df_train_features[df_train_features['is_consumption']==1].drop(columns=[\"target\"]), df_train_features[df_train_features['is_consumption']==1][\"target\"]\n","    cv     = MonthlyKFold(1)\n","    scores = cross_val_score(model, X, y, cv=cv, scoring='neg_mean_absolute_error')\n","    \n","    return -1 * np.mean(scores)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# study_c = optuna.create_study(direction='minimize', study_name='Regressor_consumption')\n","# study_c.optimize(lgb_c_objective, n_trials=10, show_progress_bar=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# study_p = optuna.create_study(direction='minimize', study_name='Regressor_production')\n","# study_p.optimize(lgb_p_objective, n_trials=10, show_progress_bar=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Train Model"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] lambda_l2 is set=10.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=10.0\n","[LightGBM] [Warning] lambda_l1 is set=1.6, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] lambda_l2 is set=10.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=10.0\n","[LightGBM] [Warning] lambda_l1 is set=1.6, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.113692 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 42102\n","[LightGBM] [Info] Number of data points in the train set: 825951, number of used features: 177\n","[LightGBM] [Info] Start training from score 110.874001\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] lambda_l2 is set=10.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=10.0\n","[LightGBM] [Warning] lambda_l1 is set=1.6, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] lambda_l2 is set=10.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=10.0\n","[LightGBM] [Warning] lambda_l1 is set=1.6, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.988947 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 42108\n","[LightGBM] [Info] Number of data points in the train set: 825951, number of used features: 177\n","[LightGBM] [Info] Start training from score 110.874001\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] lambda_l2 is set=10.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=10.0\n","[LightGBM] [Warning] lambda_l1 is set=1.6, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] lambda_l2 is set=10.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=10.0\n","[LightGBM] [Warning] lambda_l1 is set=1.6, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.307042 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 42106\n","[LightGBM] [Info] Number of data points in the train set: 825951, number of used features: 177\n","[LightGBM] [Info] Start training from score 110.874001\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] lambda_l2 is set=10.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=10.0\n","[LightGBM] [Warning] lambda_l1 is set=1.6, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] lambda_l2 is set=10.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=10.0\n","[LightGBM] [Warning] lambda_l1 is set=1.6, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.934872 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 42102\n","[LightGBM] [Info] Number of data points in the train set: 825951, number of used features: 177\n","[LightGBM] [Info] Start training from score 110.874001\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] lambda_l2 is set=10.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=10.0\n","[LightGBM] [Warning] lambda_l1 is set=1.6, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] lambda_l2 is set=10.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=10.0\n","[LightGBM] [Warning] lambda_l1 is set=1.6, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.015880 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 42104\n","[LightGBM] [Info] Number of data points in the train set: 825951, number of used features: 177\n","[LightGBM] [Info] Start training from score 110.874001\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] lambda_l2 is set=9.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.5\n","[LightGBM] [Warning] lambda_l1 is set=1.3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] lambda_l2 is set=9.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.5\n","[LightGBM] [Warning] lambda_l1 is set=1.3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.255121 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 42102\n","[LightGBM] [Info] Number of data points in the train set: 825951, number of used features: 177\n","[LightGBM] [Info] Start training from score 0.689000\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] lambda_l2 is set=9.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.5\n","[LightGBM] [Warning] lambda_l1 is set=1.3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] lambda_l2 is set=9.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.5\n","[LightGBM] [Warning] lambda_l1 is set=1.3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.308408 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 42108\n","[LightGBM] [Info] Number of data points in the train set: 825951, number of used features: 177\n","[LightGBM] [Info] Start training from score 0.689000\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] lambda_l2 is set=9.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.5\n","[LightGBM] [Warning] lambda_l1 is set=1.3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] lambda_l2 is set=9.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.5\n","[LightGBM] [Warning] lambda_l1 is set=1.3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.221893 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 42106\n","[LightGBM] [Info] Number of data points in the train set: 825951, number of used features: 177\n","[LightGBM] [Info] Start training from score 0.689000\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] lambda_l2 is set=9.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.5\n","[LightGBM] [Warning] lambda_l1 is set=1.3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] lambda_l2 is set=9.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.5\n","[LightGBM] [Warning] lambda_l1 is set=1.3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.909274 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 42102\n","[LightGBM] [Info] Number of data points in the train set: 825951, number of used features: 177\n","[LightGBM] [Info] Start training from score 0.689000\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] lambda_l2 is set=9.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.5\n","[LightGBM] [Warning] lambda_l1 is set=1.3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] lambda_l2 is set=9.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.5\n","[LightGBM] [Warning] lambda_l1 is set=1.3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.939465 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 42104\n","[LightGBM] [Info] Number of data points in the train set: 825951, number of used features: 177\n","[LightGBM] [Info] Start training from score 0.689000\n"]}],"source":["model = Model()\n","model.fit(df_train_features)\n","\n","# all_segments = df_train_features[\"segment\"].unique().tolist()\n","# segment_model = SegmentModel(all_segments)\n","# segment_model.fit(df_train_features)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] lambda_l2 is set=10.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=10.0\n","[LightGBM] [Warning] lambda_l1 is set=1.6, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] lambda_l2 is set=10.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=10.0\n","[LightGBM] [Warning] lambda_l1 is set=1.6, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.301503 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 42110\n","[LightGBM] [Info] Number of data points in the train set: 776103, number of used features: 177\n","[LightGBM] [Info] Start training from score 112.848999\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] lambda_l2 is set=10.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=10.0\n","[LightGBM] [Warning] lambda_l1 is set=1.6, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6\n","Fit Time(s): 17.791\n","Score Time(s): 0.059\n","Error(MAE): 302.844\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] lambda_l2 is set=9.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.5\n","[LightGBM] [Warning] lambda_l1 is set=1.3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] lambda_l2 is set=9.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.5\n","[LightGBM] [Warning] lambda_l1 is set=1.3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.010730 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 42110\n","[LightGBM] [Info] Number of data points in the train set: 776103, number of used features: 177\n","[LightGBM] [Info] Start training from score 0.521000\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] lambda_l2 is set=9.5, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.5\n","[LightGBM] [Warning] lambda_l1 is set=1.3, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3\n","Fit Time(s): 17.242\n","Score Time(s): 0.062\n","Error(MAE): 308.348\n"]}],"source":["model.cross_validate(df_train_features)"]},{"cell_type":"markdown","metadata":{},"source":["# Submit API"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import enefit\n","\n","env = enefit.make_env()\n","iter_test = env.iter_test()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for (\n","    df_test, \n","    df_new_target, \n","    df_new_client, \n","    df_new_historical_weather,\n","    df_new_forecast_weather, \n","    df_new_electricity_prices, \n","    df_new_gas_prices, \n","    df_sample_prediction\n",") in iter_test:\n","\n","    data_storage.update_with_new_data(\n","        df_new_client=df_new_client,\n","        df_new_gas_prices=df_new_gas_prices,\n","        df_new_electricity_prices=df_new_electricity_prices,\n","        df_new_forecast_weather=df_new_forecast_weather,\n","        df_new_historical_weather=df_new_historical_weather,\n","        df_new_target=df_new_target\n","    )\n","    df_test = data_storage.preprocess_test(df_test)\n","    \n","    df_test_features = features_generator.generate_features(df_test)\n","    df_sample_prediction[\"target\"] = model.predict(df_test_features)\n","    \n","    env.predict(df_sample_prediction)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":7292407,"sourceId":57236,"sourceType":"competition"}],"dockerImageVersionId":30588,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
