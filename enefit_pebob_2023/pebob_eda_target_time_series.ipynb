{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv-3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "import lightgbm as lgb\n",
    "import prophet as pr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng(df_data, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target):\n",
    "    df_data = (\n",
    "        df_data\n",
    "        .with_columns(\n",
    "            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_client = (\n",
    "        df_client\n",
    "        .with_columns(\n",
    "            (pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_gas = (\n",
    "        df_gas\n",
    "        .rename({\"forecast_date\": \"date\"})\n",
    "        .with_columns(\n",
    "            (pl.col(\"date\") + pl.duration(days=1)).cast(pl.Date)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_electricity = (\n",
    "        df_electricity\n",
    "        .rename({\"forecast_date\": \"datetime\"})\n",
    "        .with_columns(\n",
    "            pl.col(\"datetime\") + pl.duration(days=1)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_location = (\n",
    "        df_location\n",
    "        .with_columns(\n",
    "            pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "            pl.col(\"longitude\").cast(pl.datatypes.Float32)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df_forecast = (\n",
    "        df_forecast\n",
    "        .rename({\"forecast_datetime\": \"datetime\"})\n",
    "        .with_columns(\n",
    "            pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "            pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "#             pl.col('datetime').dt.convert_time_zone(\"Europe/Bucharest\").dt.replace_time_zone(None).cast(pl.Datetime(\"us\")),\n",
    "            pl.col('datetime').dt.replace_time_zone(None).cast(pl.Datetime(\"us\"))\n",
    "        )\n",
    "        .join(df_location, how=\"left\", on=[\"longitude\", \"latitude\"])\n",
    "        .drop(\"longitude\", \"latitude\")\n",
    "    )\n",
    "    \n",
    "    df_historical = (\n",
    "        df_historical\n",
    "        .with_columns(\n",
    "            pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "            pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            pl.col(\"datetime\") + pl.duration(hours=37)\n",
    "        )\n",
    "        .join(df_location, how=\"left\", on=[\"longitude\", \"latitude\"])\n",
    "        .drop(\"longitude\", \"latitude\")\n",
    "    )\n",
    "    \n",
    "    df_forecast_date = (\n",
    "        df_forecast\n",
    "        .group_by(\"datetime\").mean()\n",
    "        .drop(\"county\")\n",
    "    )\n",
    "    \n",
    "    df_forecast_local = (\n",
    "        df_forecast\n",
    "        .filter(pl.col(\"county\").is_not_null())\n",
    "        .group_by(\"county\", \"datetime\").mean()\n",
    "    )\n",
    "    \n",
    "    df_historical_date = (\n",
    "        df_historical\n",
    "        .group_by(\"datetime\").mean()\n",
    "        .drop(\"county\")\n",
    "    )\n",
    "    \n",
    "    df_historical_local = (\n",
    "        df_historical\n",
    "        .filter(pl.col(\"county\").is_not_null())\n",
    "        .group_by(\"county\", \"datetime\").mean()\n",
    "    )\n",
    "    \n",
    "    df_data = (\n",
    "        df_data\n",
    "        .join(df_gas, on=\"date\", how=\"left\")\n",
    "        .join(df_client, on=[\"county\", \"is_business\", \"product_type\", \"date\"], how=\"left\")\n",
    "        .join(df_electricity, on=\"datetime\", how=\"left\")\n",
    "        \n",
    "        .join(df_forecast_date, on=\"datetime\", how=\"left\", suffix=\"_fd\")\n",
    "        .join(df_forecast_local, on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_fl\")\n",
    "        .join(df_historical_date, on=\"datetime\", how=\"left\", suffix=\"_hd\")\n",
    "        .join(df_historical_local, on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_hl\")\n",
    "        \n",
    "        .join(df_forecast_date.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=\"datetime\", how=\"left\", suffix=\"_fdw\")\n",
    "        .join(df_forecast_local.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_flw\")\n",
    "        .join(df_historical_date.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=\"datetime\", how=\"left\", suffix=\"_hdw\")\n",
    "        .join(df_historical_local.with_columns(pl.col(\"datetime\") + pl.duration(days=7)), on=[\"county\", \"datetime\"], how=\"left\", suffix=\"_hlw\")\n",
    "        \n",
    "        # .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=1)).rename({\"target\": \"target_1\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        # .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=2)).rename({\"target\": \"target_2\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        # .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=3)).rename({\"target\": \"target_3\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        # .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=4)).rename({\"target\": \"target_4\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        # .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=5)).rename({\"target\": \"target_5\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        # .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=6)).rename({\"target\": \"target_6\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        # .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=7)).rename({\"target\": \"target_7\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        # .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=14)).rename({\"target\": \"target_14\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        # .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=21)).rename({\"target\": \"target_21\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        # .join(df_target.with_columns(pl.col(\"datetime\") + pl.duration(days=28)).rename({\"target\": \"target_28\"}), on=[\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"datetime\"], how=\"left\")\n",
    "        \n",
    "        .with_columns(\n",
    "            pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n",
    "            pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "            pl.col(\"datetime\").dt.day().alias(\"day\"),\n",
    "            pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n",
    "            pl.col(\"datetime\").dt.month().alias(\"month\"),\n",
    "            pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
    "        )\n",
    "        \n",
    "        .with_columns(\n",
    "            pl.concat_str(\"county\", \"is_business\", \"product_type\", \"is_consumption\", separator=\"_\").alias(\"category_1\"),\n",
    "        )\n",
    "        \n",
    "        .with_columns(\n",
    "            (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n",
    "            (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
    "            (np.pi * pl.col(\"day\") / 15).sin().alias(\"sin(dayofmonth)\"),\n",
    "            (np.pi * pl.col(\"day\") / 15).cos().alias(\"cos(dayofmonth)\"),\n",
    "            (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n",
    "            (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n",
    "        )\n",
    "        \n",
    "        .with_columns(\n",
    "            pl.col(pl.Float64).cast(pl.Float32),\n",
    "        )\n",
    "        \n",
    "        .with_columns(\n",
    "            pl.col(\"datetime\").alias(\"ds\"),   # Keep datetime around for time series forecasting\n",
    "        )\n",
    "\n",
    "        .drop(\"datetime\", \"date\", \"hour\", \"day\", \"dayofyear\")\n",
    "    )\n",
    "    \n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pandas(X, y=None):\n",
    "    cat_cols = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"category_1\"]\n",
    "    dt_cols = [\"ds\"]\n",
    "    \n",
    "    if y is not None:\n",
    "        df = pd.concat([X.to_pandas(), y.to_pandas()], axis=1)\n",
    "    else:\n",
    "        df = X.to_pandas()    \n",
    "    \n",
    "    df = df.set_index(\"row_id\")\n",
    "    df[cat_cols] = df[cat_cols].astype(\"category\")\n",
    "    df[dt_cols] = df[dt_cols].astype(\"datetime64[ns]\")\n",
    "    \n",
    "    # Add a new column with the predicted target from the corresponding time series model\n",
    "    # Also renames the category_1 column to use the same names as the time series models\n",
    "    df[\"category_1\"] = df[\"category_1\"].apply(lambda x: 'county{}_isBusiness{}_productType{}_isConsumption{}'.format(*x.split(\"_\")))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility cell to transfer between local and online notebook runs\n",
    "root = \"competition_data\"\n",
    "# root = \"/kaggle/input/predict-energy-behavior-of-prosumers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's fit a time series to each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot the `train.csv` data to create a time series for each category to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(root, \"train.csv\"))\n",
    "\n",
    "# Pivot the training data to have a cleaner DataFrame where we can analyze the mean target values\n",
    "# organized by datetime and various categorical variables.\n",
    "pivot_train = train.pivot_table(index='datetime',columns=['county','is_business','product_type','is_consumption'], values='target', aggfunc='mean')\n",
    "\n",
    "# Renaming columns for easier access and interpretation\n",
    "pivot_train.columns = ['county{}_isBusiness{}_productType{}_isConsumption{}'.format(*col) for col in pivot_train.columns.values]\n",
    "pivot_train.index = pd.to_datetime(pivot_train.index)\n",
    "pivot_train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit one instance of the Prophet model to each category time series. This operation will take about 15 minutes total for the 138 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:13:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:13:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:13:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:13:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:13:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:13:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:13:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:14:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:14:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:14:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:14:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:14:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:14:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:14:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:14:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:14:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:14:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:14:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:14:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:14:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:14:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:14:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:14:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:14:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:14:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:14:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:14:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:14:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:14:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:14:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:14:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:14:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:15:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:15:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:15:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:15:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:15:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:15:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:15:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:15:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:15:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:15:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:15:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:15:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:15:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:15:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:15:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:16:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:16:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:16:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:16:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:16:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:16:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:16:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:16:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:16:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:16:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:16:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:16:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:16:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:16:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:16:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:16:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:17:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:17:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:17:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:17:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:17:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:17:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:17:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:17:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:17:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:17:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:17:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:17:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:17:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:17:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:17:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:17:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:18:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:18:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:18:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:18:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:18:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:18:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:18:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:18:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:18:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:18:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:18:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:18:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:18:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:18:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:18:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:18:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:18:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:18:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:19:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:19:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:19:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:19:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:19:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:19:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:19:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:19:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:19:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:19:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:19:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:19:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:19:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:19:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:19:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:19:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:19:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:19:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:19:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:19:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:19:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:20:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:20:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:20:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:20:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:20:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:20:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:20:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:20:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:20:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:20:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:20:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:20:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:20:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:20:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:20:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:21:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:21:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:21:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:21:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:21:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:21:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:21:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:21:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:21:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:21:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:21:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:21:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:22:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:22:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:22:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:22:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:22:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:22:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:22:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:22:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:22:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:22:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:22:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:22:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:22:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:22:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:23:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:23:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:23:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:23:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:23:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:23:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:23:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:23:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:23:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:23:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:23:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:23:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:23:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:23:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:24:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:24:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:24:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:24:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:24:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:24:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:24:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:24:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:24:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:24:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:24:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:24:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:24:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:25:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:25:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:25:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:25:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:25:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:25:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:25:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:25:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:25:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:25:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:25:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:25:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:25:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:25:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:25:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:25:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:25:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:25:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:25:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:25:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:25:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:25:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:25:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:25:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:25:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:25:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:26:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:26:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:26:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:26:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:26:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:26:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:26:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:26:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:26:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:26:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:26:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:26:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:26:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:26:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:26:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:26:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:26:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:26:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:26:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:26:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:26:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:26:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:26:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:26:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:26:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:26:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:26:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:27:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:27:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:27:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:27:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:27:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:27:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:27:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:27:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:27:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:27:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:27:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:27:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:27:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:27:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:27:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:27:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:27:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:27:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:28:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:28:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:28:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:28:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:28:16 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "source": [
    "ts_model_dict = {}\n",
    "for col in pivot_train.columns.values:\n",
    "    if col == 'datetime':\n",
    "        continue\n",
    "\n",
    "    ts_model_dict[col] = pr.Prophet(seasonality_mode='additive', \n",
    "           yearly_seasonality=True,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=True,\n",
    "            growth = 'logistic'\n",
    "    )\n",
    "    model_df = pivot_train[['datetime',col]].rename(columns={'datetime':'ds',col:'y'})\n",
    "    model_df['cap'] = 1500.0\n",
    "    model_df['floor'] = 0.0\n",
    "    ts_model_dict[col].fit(model_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and perform feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols        = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime', 'row_id']\n",
    "client_cols      = ['product_type', 'county', 'eic_count', 'installed_capacity', 'is_business', 'date']\n",
    "gas_cols         = ['forecast_date', 'lowest_price_per_mwh', 'highest_price_per_mwh']\n",
    "electricity_cols = ['forecast_date', 'euros_per_mwh']\n",
    "forecast_cols    = ['latitude', 'longitude', 'hours_ahead', 'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low', 'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component', '10_metre_v_wind_component', 'forecast_datetime', 'direct_solar_radiation', 'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation']\n",
    "historical_cols  = ['datetime', 'temperature', 'dewpoint', 'rain', 'snowfall', 'surface_pressure','cloudcover_total','cloudcover_low','cloudcover_mid','cloudcover_high','windspeed_10m','winddirection_10m','shortwave_radiation','direct_solar_radiation','diffuse_radiation','latitude','longitude']\n",
    "location_cols    = ['longitude', 'latitude', 'county']\n",
    "target_cols      = ['target', 'county', 'is_business', 'product_type', 'is_consumption', 'datetime']\n",
    "\n",
    "save_path = None # \"trained_gbm_models_20231213.pkl\"\n",
    "load_path = None # \"trained_gbm_models.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data        = pl.read_csv(os.path.join(root, \"train.csv\"), columns=data_cols, try_parse_dates=True)\n",
    "df_client      = pl.read_csv(os.path.join(root, \"client.csv\"), columns=client_cols, try_parse_dates=True)\n",
    "df_gas         = pl.read_csv(os.path.join(root, \"gas_prices.csv\"), columns=gas_cols, try_parse_dates=True)\n",
    "df_electricity = pl.read_csv(os.path.join(root, \"electricity_prices.csv\"), columns=electricity_cols, try_parse_dates=True)\n",
    "df_forecast    = pl.read_csv(os.path.join(root, \"forecast_weather.csv\"), columns=forecast_cols, try_parse_dates=True)\n",
    "df_historical  = pl.read_csv(os.path.join(root, \"historical_weather.csv\"), columns=historical_cols, try_parse_dates=True)\n",
    "df_location    = pl.read_csv(os.path.join(root, \"weather_station_to_county_mapping.csv\"), columns=location_cols, try_parse_dates=True)\n",
    "df_target      = df_data.select(target_cols)\n",
    "\n",
    "schema_data        = df_data.schema\n",
    "schema_client      = df_client.schema\n",
    "schema_gas         = df_gas.schema\n",
    "schema_electricity = df_electricity.schema\n",
    "schema_forecast    = df_forecast.schema\n",
    "schema_historical  = df_historical.schema\n",
    "schema_target      = df_target.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_data.drop(\"target\"), df_data.select(\"target\")\n",
    "\n",
    "X = feature_eng(X, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target)\n",
    "\n",
    "df_train = to_pandas(X, y)\n",
    "\n",
    "# Optional: clip data to the last year\n",
    "# df_train = df_train[df_train[\"target\"].notnull() & df_train[\"year\"].gt(2021)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Prophet models to predict the target value\n",
    "\n",
    "This model should produce an interesting result regardless of the input time. Run time is about 4 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_series_predictions(df, model_dict=ts_model_dict):\n",
    "    df[[\"target_predicted\", \"target_predicted_lower\", \"target_predicted_upper\"]] = 0.0\n",
    "    for model_key in model_dict.keys():\n",
    "        if model_key in df[\"category_1\"]:\n",
    "            key_df = df.loc[df[\"category_1\"] == model_key]\n",
    "            prediction_df = key_df[[\"ds\"]]\n",
    "            prediction_df.loc[:, \"floor\"] = 0.0\n",
    "            prediction_df.loc[:, \"cap\"] = 1500.0\n",
    "            out_df = model_dict[model_key].predict(prediction_df)\n",
    "            df.loc[df[\"category_1\"] == model_key, [\"target_predicted\", \"target_predicted_lower\", \"target_predicted_upper\"]] = out_df[[\"yhat\", \"yhat_lower\", \"yhat_upper\"]].values\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = get_time_series_predictions(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'competition_data.enefit.competition'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcompetition_data\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menefit\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01menefit\u001b[39;00m\n\u001b[1;32m      3\u001b[0m env \u001b[38;5;241m=\u001b[39m enefit\u001b[38;5;241m.\u001b[39mmake_env()\n\u001b[1;32m      4\u001b[0m iter_test \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39miter_test()\n",
      "File \u001b[0;32m~/kaggling/enefit_pebob_2023/competition_data/enefit/__init__.py:2\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompetition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_env\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmake_env\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'competition_data.enefit.competition'"
     ]
    }
   ],
   "source": [
    "import competition_data.enefit as enefit\n",
    "\n",
    "env = enefit.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (test, revealed_targets, client, historical_weather,\n",
    "        forecast_weather, electricity_prices, gas_prices, sample_prediction) in iter_test:\n",
    "    \n",
    "    test = test.rename(columns={\"prediction_datetime\": \"datetime\"})\n",
    "    \n",
    "    df_test           = pl.from_pandas(test[data_cols[1:]], schema_overrides=schema_data)\n",
    "    df_client         = pl.from_pandas(client[client_cols], schema_overrides=schema_client)\n",
    "    df_gas            = pl.from_pandas(gas_prices[gas_cols], schema_overrides=schema_gas)\n",
    "    df_electricity    = pl.from_pandas(electricity_prices[electricity_cols], schema_overrides=schema_electricity)\n",
    "    df_new_forecast   = pl.from_pandas(forecast_weather[forecast_cols], schema_overrides=schema_forecast)\n",
    "    df_new_historical = pl.from_pandas(historical_weather[historical_cols], schema_overrides=schema_historical)\n",
    "    df_new_target     = pl.from_pandas(revealed_targets[target_cols], schema_overrides=schema_target)\n",
    "    \n",
    "    df_forecast       = pl.concat([df_forecast, df_new_forecast]).unique()\n",
    "    df_historical     = pl.concat([df_historical, df_new_historical]).unique()\n",
    "    df_target         = pl.concat([df_target, df_new_target]).unique()\n",
    "    \n",
    "    X_test = feature_eng(df_test, df_client, df_gas, df_electricity, df_forecast, df_historical, df_location, df_target)\n",
    "    X_test = to_pandas(X_test)\n",
    "    \n",
    "    # test['target'] = model.predict(X_test).clip(0)\n",
    "    # test['target_solar'] = model_solar.predict(X_test).clip(0)\n",
    "    # test.loc[test['is_consumption']==0, \"target\"] = test.loc[test['is_consumption']==0, \"target_solar\"]    \n",
    "    \n",
    "    X_out = get_time_series_predictions(X_test)\n",
    "\n",
    "    sample_prediction[\"target\"] = X_out[\"target_predicted\"].clip(0.0)\n",
    "\n",
    "    \n",
    "    env.predict(sample_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
